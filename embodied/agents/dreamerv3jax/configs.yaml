defaults:

  # Trainer
  logdir: /dev/null
  seed: 0
  task: dummy_disc
  env: {amount: 16, parallel: process, repeat: 1, size: [64, 64], camera: -1, gray: False, length: 0, discretize: 0, lives: False, sticky: True, noops: 0, episodic: True, restart: True, checks: False, resets: True, reward_scale: 1.0, seed: 0}
  replay: uniform
  replay_size: 1e6
  replay_online: False
  jax: {platform: gpu, jit: True, precision: float16, prealloc: True, debug_nans: False, parallel: False, logical_cpus: 0, debug: False}
  eval_dir: ''
  filter: '.*'
  run:
    script: train
    steps: 1e10
    expl_until: 0
    log_every: 300
    save_every: 900
    eval_every: 1e6
    eval_eps: 1
    eval_samples: 1
    train_ratio: 64
    batch_steps: -1
    train_fill: 1024
    eval_fill: 1024
    pretrain: 1
    log_zeros: False
    log_keys_video: [image]
    log_keys_sum: '^$'
    log_keys_mean: '^$'
    log_keys_max: '^$'
    sync_every: 120

  # Agent
  task_behavior: Greedy
  expl_behavior: None
  transform_rewards: off
  batch_size: 16
  batch_length: 64
  data_loaders: 8
  expl_noise: 0.0
  eval_noise: 0.0
  priority: reward_loss
  priority_correct: 0.0
  scale_images: 1

  # World Model
  grad_heads: [decoder, reward, cont]
  rssm_type: rssm
  rssm: {deter: 4096, units: 1024, stoch: 32, classes: 32, act: silu, norm: layer, initial: learned, unimix: 0.01, dynpost: True, sepdyn: False, argmax: False, unroll: False, action_clip: 1.0, smooth: 0.0, winit: normal, fan: avg, gru_depth: 0}
  group_rssm: {size: 8192, deter: 8192, groups: 32, heads: 8, stoch: 32, classes: 32, act: silu, norm: layer, initial: learned, unimix: 0.01, dynpost: True, sepdyn: False, argmax: False, unroll: False, action_clip: 1.0, smooth: 0.0, winit: normal, fan: avg}
  vqgru: {units: 1024, deter: 1024, stoch: 32, classes: 256, embed: 16, act: silu, norm: layer, dynpost: False, sepdyn: True, unroll: False, winit: normal, fan: avg}
  stgru: {units: 1024, deter: 1024, stoch: 32, classes: 32, act: silu, norm: layer, dynpost: False, sepdyn: True, unroll: False, winit: normal, fan: avg}
  encoder: {mlp_keys: '.*', cnn_keys: '.*', act: silu, norm: layer, mlp_layers: 5, mlp_units: 1024, cnn: resnet, cnn_depth: 96, cnn_kernels: [3, 3, 3, 3], cnn_blocks: 0, resize: stride, winit: normal, fan: avg}
  decoder: {mlp_keys: '.*', cnn_keys: '.*', act: silu, norm: layer, mlp_layers: 5, mlp_units: 1024, cnn: resnet, cnn_depth: 96, cnn_kernels: [3, 3, 3, 3], cnn_blocks: 0, image_dist: mse, vector_dist: symlog_mse, inputs: [deter, stoch], resize: stride, winit: normal, fan: avg, outscale: 1.0, bins: 255}
  reward_head: {layers: 5, units: 1024, act: silu, norm: layer, dist: symlog_disc, outscale: 0.0, outnorm: False, inputs: [deter, stoch], winit: normal, fan: avg, bins: 255}
  cont_head: {layers: 5, units: 1024, act: silu, norm: layer, dist: binary, outscale: 1.0, outnorm: False, inputs: [deter, stoch], winit: normal, fan: avg}
  loss_scales: {image: 1.0, reward: 1.0, cont: 1.0, kl: 1.0, dyn: 0.5, rep: 0.1, qhead: 1.0}
  model_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100.0, wd: 0.0, warmup: 0, lateclip: 0.0}
  dyn_loss: {impl: kl}
  rep_loss: {impl: kl, free: 2.0, commit: 0.25}
  kl_combine: False
  kl_balance: 0.8
  wmkl: {impl: mult, scale: 0.1, target: 3.5, min: 1e-3, max: 1.0, vel: 0.1}
  repauto: {impl: fixed, scale: 1.0, target: 5.0, min: 1e-6, max: 1.0, vel: 0.1}
  drop_loss: 0.0
  use_qhead: False
  qhead: {layers: 5, units: 1024, act: silu, norm: layer, dist: symlog_disc, outscale: 0.0, outnorm: False, inputs: [deter, stoch, action], winit: normal, fan: avg, bins: 255}
  qhead_lambda: 0.5

  # Actor Critic
  actor: {layers: 5, units: 1024, act: silu, norm: layer, minstd: 0.1, maxstd: 1.0, outscale: 1.0, outnorm: False, unimix: 0.01, rawstd_offset: 0.0, inputs: [deter, stoch], winit: normal, fan: avg}
  critic: {layers: 5, units: 1024, act: silu, norm: layer, dist: symlog_disc, outscale: 0.0, outnorm: False, inputs: [deter, stoch], winit: normal, fan: avg, bins: 255}
  slow_critic_zero_init: False
  actor_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100.0, wd: 0.0, warmup: 0, lateclip: 0.0}
  critic_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100.0, wd: 0.0, warmup: 0, lateclip: 0.0}
  actor_dist_disc: onehot
  actor_dist_cont: normal
  actor_grad_disc: reinforce
  actor_grad_cont: backprop
  critic_type: vfunction
  imag_horizon: 15
  imag_unroll: False
  horizon: 333
  return_lambda: 0.95
  actor_return_impl: gve
  critic_return_impl: gve
  actor_return_slow: True
  critic_return_slow: True
  slow_critic: True
  slow_actor: False
  slow_critic_update: 50  # 100
  slow_critic_fraction: 1.0
  slow_actor_update: 1  # 100
  slow_actor_fraction: 0.01  # 1.0
  slow_target_min: False
  slow_target_clip: 0.0
  critic_trust: 0.0
  actent: {impl: fixed, scale: 2e-4, target: 0.4, min: 1e-3, max: 3e-2, vel: 0.1}
  rewmom: {impl: mean_std, decay: 0.99, max: 1e2}
  retmom: {impl: min_max, decay: 0.99, max: 1.0}
  advmom: {impl: mean_std, decay: 0.99, max: 1e8}
  rewnorm: False
  rewnorm_horizon: False
  retnorm: return
  advnorm: False
  entnorm: False
  retnorm_horizon: False
  slow_kl: 0.0
  imag_reward_min: 0.0  # 1e-3
  adv_symlog: False
  adv_clip: 3.0
  adv_temp: -1.0
  dueling_critic: False
  muesli: 0
  actor_smooth: 0.0

  # Exploration
  expl_rewards: {extr: 0.0, disag: 0.0, vae: 0.0, ctrl: 0.0, pbe: 0.0}
  expl_discount: 0.99
  expl_retnorm: {impl: mean_std, decay: 0.999, max: 1e8}
  expl_advnorm: {impl: off, decay: 0.999, max: 1e8}
  expl_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100.0, wd: 0.0, warmup: 0, lateclip: 0.0}
  disag_head: {layers: 5, units: 1024, act: silu, norm: layer, dist: mse, outscale: 1.0, inputs: [deter, stoch, action], winit: normal, fan: avg}
  disag_target: [stoch]
  disag_models: 8
  ctrl_embed: {layers: 3, units: 1024, act: silu, norm: layer, dist: mse, inputs: [deter, stoch]}
  ctrl_head: {layers: 1, units: 128, act: silu, norm: layer, dist: mse, inputs: [current, next]}
  ctrl_size: 32
  ctrl_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100.0, wd: 0.0, warmup: 0, lateclip: 0.0}
  pbe_inputs: [deter]
  pbe_knn: 16
  expl_enc: {layers: 5, units: 1024, act: silu, norm: layer, dist: onehot, outscale: 1.0, outnorm: False, inputs: [deter], shape: [8, 8], winit: normal, fan: avg}
  expl_dec: {layers: 5, units: 1024, act: silu, norm: layer, dist: mse, outscale: 1.0, outnorm: False, winit: normal, fan: avg}
  expl_kl: {impl: mult, scale: 0.1, target: 10.0, min: 0.0, max: 1.0, vel: 0.1}

minecraft:

  task: minecraft_discover
  # env: {amount: 16, parallel: thread}
  run:
    eval_fill: 1e5
    train_ratio: 16
    log_keys_max: '^log_inventory.*'
  encoder: {mlp_keys: 'inventory|equipped|reward|is_terminal', cnn_keys: 'image'}
  decoder: {mlp_keys: 'inventory|equipped', cnn_keys: 'image'}
  # jax.precision: float32

dmlab:

  task: dmlab_rooms_collect_good_objects_train
  env: {repeat: 4, episodic: False, amount: 16}
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

atari:

  task: atari_pong
  env: {gray: True, repeat: 4, amount: 16}
  run:
    # script: train_eval
    steps: 5.5e7
    eval_eps: 10
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

atari100k:

  task: atari_pong
  env:
    gray: False
    repeat: 4
    sticky: False
    length: 3000
    lives: True
    noops: 30
    amount: 1
    # size: [84, 84]  # seems to help but very slow?
  run:
    script: train_eval
    steps: 1.2e5
    eval_every: 1e5
    eval_eps: 32
    train_ratio: 512
  jax.precision: float32
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

crafter:

  task: crafter_reward
  run:
    log_keys_max: '^log_achievement_.*'
    log_keys_sum: '^log_reward$'
  run.train_ratio: 512
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

procgen:

  task: procgen_coinrun
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

dmc_vision:

  task: dmc_walker_walk
  env.amount: 8
  env.repeat: 2
  rssm.deter: 2048  # For dynamics backprop to fit into VRAM.
  run.train_ratio: 512
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

dmc_proprio:

  task: dmc_walker_walk
  env.amount: 8
  env.repeat: 2
  rssm.deter: 2048  # For dynamics backprop to fit into VRAM.
  run.train_ratio: 512
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}

robodesk:

  task: robodesk_push_slide_dense
  env: {amount: 8, repeat: 4, length: 2000}
  run:
    script: train_eval
    eval_eps: 8
    train_ratio: 256
  encoder: {mlp_keys: '^$', cnn_keys: 'image'}
  decoder: {mlp_keys: '^$', cnn_keys: 'image'}
  discount: 0.99

loconav_ant:

  task: loconav_ant_umaze
  env.repeat: 2
  run:
    train_ratio: 512
    log_keys_max: '^log_.*'
  encoder: {mlp_keys: '.*', cnn_keys: 'image'}
  decoder: {mlp_keys: '.*', cnn_keys: 'image'}

hrlgrid:

  task: hrlgrid_16
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

small:

  env.amount: 2
  run:
    eval_every: 1e4
    log_every: 20
    train_fill: 100
    eval_fill: 100
  # jax.precision: float32
  batch_size: 16
  batch_length: 16
  rssm.deter: 128
  rssm.units: 128
  rssm.stoch: 8
  .*\.cnn_depth$: 16
  .*\.wd$: 0.0

debug:

  jax: {jit: True, prealloc: False, debug: True}
  env: {length: 100, restart: False, amount: 2, checks: True}
  run:
    eval_every: 1000
    log_every: 5
    save_every: 10
    train_fill: 100
    eval_fill: 100
    train_ratio: 32
  batch_size: 8
  batch_length: 12
  replay_size: 500
  encoder.cnn_depth: 16
  decoder.cnn_depth: 16
  rssm: {units: 64, stoch: 8, classes: 8}
  .*unroll: False
  .*\.layers: 2
  .*\.units: 64
  .*\.wd$: 0.0
